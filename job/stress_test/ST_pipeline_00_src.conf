
env {
  job.name = "ST_pipeline_00_src"
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 5000
  job.retry.times = 3
  job.retry.interval.seconds = 10
  read_limit.bytes_per_second=7000000
  read_limit.rows_per_second=400
  logging.level = "DEBUG"

}

source {
  MySQL-CDC {
    base-url = "jdbc:mysql://192.168.138.16:3306/inventory01"
    username = "seatunnel_src"
    password = "Abcd1234"
    startup.mode="initial"
    format = compatible_debezium_json
    schema-changes.enabled = true
    table-names = ["inventory00.orders"]
    plugin_output = "src_table"
    debezium = {
        # include schema into kafka message
        key.converter.schemas.enable = false
        value.converter.schemas.enable = false
        # topic prefix
        database.server.name = "192.168.138.16"
    }
  }
}

sink {
  kafka {
      topic = "ST_pipeline_00_src"
      bootstrap.servers = "192.168.138.16:29092,192.168.138.16:29093"
      format = compatible_debezium_json
      kafka.request.timeout.ms = 60000
      semantics = EXACTLY_ONCE
      kafka.config = {
        acks = "all"
        request.timeout.ms = 30000
        buffer.memory = 33554432
        compression.type = lz4
        batch.size = 500
      }
  }
}