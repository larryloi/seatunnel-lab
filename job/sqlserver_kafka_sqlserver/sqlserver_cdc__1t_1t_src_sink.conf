env {
  job.name = "sqlserver_cdc__1t_1t_src_sink"
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 5000
  job.retry.times = 3
  job.retry.interval.seconds = 10
  read_limit.bytes_per_second=7000000
  read_limit.rows_per_second=400
  logging.level = "DEBUG"
}

source {
  Kafka {
    plugin_output = "emps_source"
    bootstrap.servers = "172.199.0.3:29092"
    topic = "sqlserver_cdc__1t_1t"
    consumer.group = "sqlserver_cdc__1t_1t"
    format = "debezium_json"
    debezium_record_include_schema = false
    start_mode = latest
    kafka.config = {
      auto.offset.reset = "latest"
      enable.auto.commit = "true"
      max.partition.fetch.bytes = "5242880"
      session.timeout.ms = "30000"
      max.poll.records = "100000"
    }
    schema = {
      fields {
          id = "int"
          depts = "int"
          first_name = "string"
          last_name = "string"
          hire_date = "timestamp"
      }
    }
  }

}


sink {
  Jdbc {
    plugin_input = ["emps_source"]
    driver = com.microsoft.sqlserver.jdbc.SQLServerDriver
    url = "jdbc:sqlserver://db01.kaskade.local:1433;databaseName=dwh;encrypt=false"
    user = ${SINK_DB_USER}
    password = ${SINK_DB_PASSWORD}
    generate_sink_sql = true
    database = "dwh"
    table = "dwh.DW_ETL.emps
    primary_keys = ["id"]
    schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
    enable_upsert = true
    max_retries = 3
    batch_size = 2000
  }
}

