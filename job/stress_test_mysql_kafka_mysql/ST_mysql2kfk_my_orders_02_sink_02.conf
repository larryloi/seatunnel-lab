env {
  job.name = "ST_mysql2kfk_my_orders_02_sink_02"
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 10000
}

source {
  Kafka {
    plugin_output = "fake1"

    bootstrap.servers = "broker:29092"
    topic = "ST_mysql2kfk_my_orders_02"
    consumer.group = "ST_mysql2kfk_my_orders_02_sink_02"
    format = "debezium_json"
    debezium_record_include_schema = false
    start_mode = latest
    kafka.config = {
      auto.offset.reset = "latest"
      enable.auto.commit = "true"
      max.partition.fetch.bytes = "5242880"
      session.timeout.ms = "30000"
      max.poll.records = "100000"
    }
    schema = {
      fields {
          id = "int"
          order_id = "string"
          supplier_id = "int"
          item_id = "int"
          status = "string"
          qty = "int"
          net_price = "int"
          issued_at = "int"
          completed_at = "int"
          spec = "string"
          created_at = "int"
          updated_at = "int"
      }

    }
  }
}

sink {
    jdbc {
        url = "jdbc:mysql://db01.kaskade.local:3306/DWH_Seatunnel?useUnicode=true&characterEncoding=UTF-8&rewriteBatchedStatements=true"
        driver = "com.mysql.cj.jdbc.Driver"
        user = "seatunnel_sink"
        password = "Abcd1234"
        generate_sink_sql = true
        database = DWH_Seatunnel
        table = ST_mysql2kfk_my_orders_02A
        primary_keys = ["id"]
        field_ide = ORIGINAL
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
        enable_upsert = true
    }    
}